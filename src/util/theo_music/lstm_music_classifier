import numpy as np
import pandas as pd 
import pickle as pkl
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D
train = pd.read_csv("archive/lyrics.csv")

MAX_WORDS = 10000
MAX_LENGTH = 150
# This is fixed.
EMBEDDING_DIM = 100
print(train)
tokenizer = Tokenizer(num_words=MAX_WORDS, filters='!"#$%&()*+,-./:;<=>?@', lower=True)
tokenizer.fit_on_texts(train['lyrics'].values)
word_index = tokenizer.word_index
print('tokens',  len(word_index))

X = tokenizer.texts_to_sequences(train['lyrics'].values)
X = pad_sequences(X, maxlen=MAX_LENGTH)
print(X.shape)

Y = pd.get_dummies(train['song_name']).values
print(Y.shape)

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)

print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

print("finished init")

model = Sequential()
model.add(Embedding(MAX_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(1000, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(200, activation='softmax'))
model.add(Dense(Y_test.shape[1], activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])


history = model.fit(X_train, Y_train, epochs=10, batch_size=50,validation_split=0.1)

with open("model.pkl", "wb") as file:
    pkl.dump(model, file)

print("complete")